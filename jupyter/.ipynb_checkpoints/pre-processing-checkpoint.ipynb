{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be698938",
   "metadata": {},
   "source": [
    "## Stopword removal & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67dcfee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mamieo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mamieo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mamieo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "630f7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d5848",
   "metadata": {},
   "source": [
    "#### Example tokenization and remove stopword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e037b180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Its about to be 2020 and Im panicking because I dont know what to do I get nervous thinking about the future and imagining what could happen and its really stressing me outðŸ˜“'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories['stress']['Selftext'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea1518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Its', 'about', 'to', 'be', '2020', 'and', 'Im', 'panicking', 'because', 'I', 'dont', 'know', 'what', 'to', 'do', 'I', 'get', 'nervous', 'thinking', 'about', 'the', 'future', 'and', 'imagining', 'what', 'could', 'happen', 'and', 'its', 'really', 'stressing', 'me', 'outðŸ˜“']\n"
     ]
    }
   ],
   "source": [
    "def token(submission):\n",
    "    token_words = word_tokenize(submission)\n",
    "    return token_words\n",
    "    \n",
    "terms = token(all_categories['stress']['Selftext'][0])\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ad3d943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', '2020', 'im', 'panick', 'i', 'dont', 'know', 'i', 'get', 'nervou', 'think', 'futur', 'imagin', 'could', 'happen', 'realli', 'stress', 'outðŸ˜“']\n"
     ]
    }
   ],
   "source": [
    "def stem_tokens(terms):\n",
    "    porter = PorterStemmer()\n",
    "    stem_terms = []\n",
    "    for term in terms:\n",
    "        stem_terms.append(porter.stem(term))\n",
    "    return stem_terms;\n",
    "\n",
    "def lemma_tokens(terms):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_terms = []\n",
    "\n",
    "    for term in terms:\n",
    "        lemma_terms.append(wordnet_lemmatizer.lemmatize(term))\n",
    "    return lemma_terms;\n",
    "\n",
    "def remove_stopword(terms):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokens_without_sw = [word for word in terms if not word in stop_words]\n",
    "    return tokens_without_sw\n",
    "    \n",
    "lemma_terms = lemma_tokens(terms)\n",
    "stem_terms = stem_tokens(lemma_terms)\n",
    "terms = remove_stopword(terms)\n",
    "\n",
    "print(stem_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6682df",
   "metadata": {},
   "source": [
    "#### def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_single_characters(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc6230",
   "metadata": {},
   "source": [
    "1. No need to convert lower case because stemming and lemma will do it\n",
    "2. Punctuation is the set of unnecessary symbols that are in our corpus documents.\n",
    "3. Note that there is no â€˜ apostrophe in the punctuation symbols. Because when we remove punctuation first it will convert donâ€™t to dont, and it is a stop word that won't be removed.\n",
    "4. Single characters are not much useful in knowing the importance of the document and few final single characters might be irrelevant symbols\n",
    "5. Stemming, playing and played are the same type of words that basically indicate an action play.\n",
    "6. Lemmatisation is a way to reduce the word to the root synonym of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b66d81",
   "metadata": {},
   "source": [
    "### Thus, \n",
    "#### if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n",
    "\n",
    "- The most significant word for **document A** is man and walk\n",
    "- The most significant word for **document B** is around, children, fire, and sat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef508bea",
   "metadata": {},
   "source": [
    "# Preprocessing - Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd54f35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tf2/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'m</th>\n",
       "      <th>,</th>\n",
       "      <th>2020</th>\n",
       "      <th>abl</th>\n",
       "      <th>activ</th>\n",
       "      <th>allow</th>\n",
       "      <th>also</th>\n",
       "      <th>ani</th>\n",
       "      <th>appli</th>\n",
       "      <th>arent</th>\n",
       "      <th>...</th>\n",
       "      <th>think</th>\n",
       "      <th>though</th>\n",
       "      <th>time</th>\n",
       "      <th>tri</th>\n",
       "      <th>wa</th>\n",
       "      <th>week</th>\n",
       "      <th>wont</th>\n",
       "      <th>worri</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.161601</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.122902</td>\n",
       "      <td>0.061451</td>\n",
       "      <td>0.323202</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.061451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100399</td>\n",
       "      <td>0.152712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152712</td>\n",
       "      <td>0.076356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305425</td>\n",
       "      <td>0.200798</td>\n",
       "      <td>0.076356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         'm         ,      2020       abl     activ     allow      also  \\\n",
       "0  0.000000  0.000000  0.286675  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.491607  0.000000  0.080801  0.080801  0.000000  0.080801   \n",
       "2  0.100399  0.152712  0.000000  0.000000  0.000000  0.100399  0.000000   \n",
       "\n",
       "        ani     appli     arent  ...     think    though      time       tri  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.218024  0.000000  0.000000  0.000000   \n",
       "1  0.161601  0.080801  0.080801  ...  0.000000  0.080801  0.122902  0.061451   \n",
       "2  0.000000  0.000000  0.000000  ...  0.076356  0.000000  0.152712  0.076356   \n",
       "\n",
       "         wa      week      wont     worri     would      year  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.323202  0.080801  0.080801  0.061451  0.000000  0.061451  \n",
       "2  0.000000  0.000000  0.000000  0.305425  0.200798  0.076356  \n",
       "\n",
       "[3 rows x 125 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission1 = all_categories['stress']['Selftext'][0]\n",
    "submission2 = all_categories['stress']['Selftext'][1]\n",
    "submission3 = all_categories['stress']['Selftext'][2]\n",
    "\n",
    "submissions = [submission1, submission2, submission3]\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemma_terms = lemma_tokens(tokens)\n",
    "    stem_terms = stem_tokens(lemma_terms)\n",
    "    terms = remove_stopword(stem_terms)\n",
    "    return terms\n",
    "\n",
    "# vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "# submission_vectors = vectorizer.fit_transform(submissions)\n",
    "\n",
    "# dense = submission_vectors.todense()\n",
    "# submission_list = dense.tolist()\n",
    "\n",
    "df = pd.DataFrame(submission_list, columns=vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3bd1f903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'m</th>\n",
       "      <th>,</th>\n",
       "      <th>2020</th>\n",
       "      <th>abl</th>\n",
       "      <th>activ</th>\n",
       "      <th>allow</th>\n",
       "      <th>also</th>\n",
       "      <th>ani</th>\n",
       "      <th>appli</th>\n",
       "      <th>arent</th>\n",
       "      <th>...</th>\n",
       "      <th>think</th>\n",
       "      <th>though</th>\n",
       "      <th>time</th>\n",
       "      <th>tri</th>\n",
       "      <th>wa</th>\n",
       "      <th>week</th>\n",
       "      <th>wont</th>\n",
       "      <th>worri</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100399</td>\n",
       "      <td>0.152712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152712</td>\n",
       "      <td>0.076356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305425</td>\n",
       "      <td>0.200798</td>\n",
       "      <td>0.076356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.161601</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.122902</td>\n",
       "      <td>0.061451</td>\n",
       "      <td>0.323202</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.080801</td>\n",
       "      <td>0.061451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         'm         ,      2020       abl     activ     allow      also  \\\n",
       "2  0.100399  0.152712  0.000000  0.000000  0.000000  0.100399  0.000000   \n",
       "1  0.000000  0.491607  0.000000  0.080801  0.080801  0.000000  0.080801   \n",
       "0  0.000000  0.000000  0.286675  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        ani     appli     arent  ...     think    though      time       tri  \\\n",
       "2  0.000000  0.000000  0.000000  ...  0.076356  0.000000  0.152712  0.076356   \n",
       "1  0.161601  0.080801  0.080801  ...  0.000000  0.080801  0.122902  0.061451   \n",
       "0  0.000000  0.000000  0.000000  ...  0.218024  0.000000  0.000000  0.000000   \n",
       "\n",
       "         wa      week      wont     worri     would      year  \n",
       "2  0.000000  0.000000  0.000000  0.305425  0.200798  0.076356  \n",
       "1  0.323202  0.080801  0.080801  0.061451  0.000000  0.061451  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 125 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['worri'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "efca49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c51c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its about to be 2020 and Im panicking because I dont know what to do I get nervous thinking about the future and imagining what could happen and its really stressing me outðŸ˜“\n",
      "====\n",
      "['2020', 'im', 'panick', 'becaus', 'dont', 'know', 'get', 'nervou', 'think', 'futur', 'imagin', 'could', 'happen', 'realli', 'stress', 'outðŸ˜“']\n",
      "=======\n",
      "2020 im panick becaus dont know get nervou think futur imagin could happen realli stress outðŸ˜“\n",
      "Im a sophomore in college, and last semester was super busy I immersed myself in a lot of activities last year, mostly because I was really depressed and needed things to do on campus Ive enjoyed all the things Im a part of now though, but it definitely takes up a lot of time I should have searched for more internships last semester, but I procrastinated and was also sick all the time I applied for a big internship that was through my school, but I didnt get it, and I didnt look into any other internshipsIm going back to school in a couple weeks and the stress is really settling in There havent been many new postings and there arent any for the summer I keep trying to start researching but I panic so much and the stress eats at me Im just extremely worried for the future, that I wont be able to get an internship or a job, or that Ill be falling behind\n",
      "====\n",
      "['im', 'sophomor', 'colleg', ',', 'last', 'semest', 'wa', 'super', 'busi', 'immers', 'lot', 'activ', 'last', 'year', ',', 'mostli', 'becaus', 'wa', 'realli', 'depress', 'need', 'thing', 'campu', 'ive', 'enjoy', 'thing', 'im', 'part', 'though', ',', 'definit', 'take', 'lot', 'time', 'search', 'internship', 'last', 'semest', ',', 'procrastin', 'wa', 'also', 'sick', 'time', 'appli', 'big', 'internship', 'wa', 'school', ',', 'didnt', 'get', ',', 'didnt', 'look', 'ani', 'internshipsim', 'go', 'back', 'school', 'coupl', 'week', 'stress', 'realli', 'settl', 'havent', 'mani', 'new', 'post', 'arent', 'ani', 'summer', 'keep', 'tri', 'start', 'research', 'panic', 'much', 'stress', 'eat', 'im', 'extrem', 'worri', 'futur', ',', 'wont', 'abl', 'get', 'internship', 'job', ',', 'ill', 'fall', 'behind']\n",
      "=======\n",
      "im sophomor colleg , last semest wa super busi immers lot activ last year , mostli becaus wa realli depress need thing campu ive enjoy thing im part though , definit take lot time search internship last semest , procrastin wa also sick time appli big internship wa school , didnt get , didnt look ani internshipsim go back school coupl week stress realli settl havent mani new post arent ani summer keep tri start research panic much stress eat im extrem worri futur , wont abl get internship job , ill fall behind\n",
      "this year is my major exams I just came back from a long school holiday but I feel drained really drained I can't think straight, I'm making mistakes during homework I have tried playing a game of call of duty but just seem to continue being bothered Im constantly worrying which deprives me of sleep The major exams thing is just bugging me I even started on more enrichment classes to help to stop me from worrying even more The only time I wouldn't worry is with my best friend But I just feel my teachers wouldn't allow me to be seated with him He distracts me from all my worries With my schedule being so packed now, I don't know if I can spend time with him idk what to do now thanks for listening me rant\n",
      "====\n",
      "['thi', 'year', 'major', 'exam', 'came', 'back', 'long', 'school', 'holiday', 'feel', 'drain', 'realli', 'drain', 'ca', \"n't\", 'think', 'straight', ',', \"'m\", 'make', 'mistak', 'dure', 'homework', 'tri', 'play', 'game', 'call', 'duti', 'seem', 'continu', 'bother', 'im', 'constantli', 'worri', 'depriv', 'sleep', 'major', 'exam', 'thing', 'bug', 'even', 'start', 'enrich', 'class', 'help', 'stop', 'worri', 'even', 'onli', 'time', 'would', \"n't\", 'worri', 'best', 'friend', 'feel', 'teacher', 'would', \"n't\", 'allow', 'seat', 'distract', 'worri', 'schedul', 'pack', ',', \"n't\", 'know', 'spend', 'time', 'idk', 'thank', 'listen', 'rant']\n",
      "=======\n",
      "thi year major exam came back long school holiday feel drain realli drain ca n't think straight , 'm make mistak dure homework tri play game call duti seem continu bother im constantli worri depriv sleep major exam thing bug even start enrich class help stop worri even onli time would n't worri best friend feel teacher would n't allow seat distract worri schedul pack , n't know spend time idk thank listen rant\n"
     ]
    }
   ],
   "source": [
    "for submission in submissions:\n",
    "    print(submission)\n",
    "    print(\"====\")\n",
    "    terms = tokenize(submission)\n",
    "    print(terms)\n",
    "    print(\"=======\")\n",
    "    \n",
    "    text = \" \".join(terms)\n",
    "    print(text)\n",
    "    print(\"#####\")\n",
    "\n",
    "# df = pd.DataFrame(submission_list, columns=vectorizer.get_feature_names())\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd0be1",
   "metadata": {},
   "source": [
    "##  Summary without Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59df17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
